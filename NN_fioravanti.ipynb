{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_fioravanti.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogggqj_5VrI8",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "Google Drive Connection\n",
        "\n",
        "Import required libraries\n",
        "\n",
        "Keras functions dictionaries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMurOUMrks5s",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers \\\n",
        "    import Flatten, Dense, Activation, GaussianNoise, BatchNormalization, InputLayer, Dropout\n",
        "\n",
        "from time import time, gmtime, strftime\n",
        "import os, json, itertools\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "#from plot_job import plot_job\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus: print(f'GPU device(s): {gpus}')\n",
        "else: print('GPU not found')\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "with strategy.scope():\n",
        "  ## replicas\n",
        "  print('Strategy replicas:',strategy.num_replicas_in_sync)\n",
        "\n",
        "  ## loss\n",
        "  LOSS = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "  loss_mean = tf.keras.metrics.Mean(name='loss_mean')\n",
        "  ## activation function and derivative\n",
        "  f_ACT = {\n",
        "      'sigmoid': tf.keras.activations.sigmoid,\n",
        "      'tanh': tf.keras.activations.tanh,\n",
        "      'relu': tf.keras.activations.relu}\n",
        "  f_ACT_der = {\n",
        "    'sigmoid': lambda x: 1/(1+np.e**-x),\n",
        "    'tanh': lambda x: 1-((np.e**x-np.e**-x)/(np.e**x+np.e**-x))**2,\n",
        "    'relu': lambda x: np.where(x<=0,0,1)}\n",
        "  ## metric\n",
        "  f_METRIC = {\n",
        "    'accuracy': tf.keras.metrics.SparseCategoricalAccuracy,\n",
        "    'crossentropy': tf.keras.metrics.SparseCategoricalCrossentropy}\n",
        "  METRIC1 = f_METRIC['accuracy'](name='metric_train')\n",
        "  METRIC2 = f_METRIC['accuracy'](name='metric_test')\n",
        "  ## optimizer\n",
        "  f_OPT = {\n",
        "    'adam': tf.keras.optimizers.Adam,\n",
        "    'sgd': tf.keras.optimizers.SGD}\n",
        "\n",
        "  ## regularizer\n",
        "  RGLRZ = tf.keras.regularizers.l2\n",
        "\n",
        "  ## zero initializer for tf.Variable\n",
        "  ZERO = tf.Variable(0., trainable=False, name='zero')\n",
        "\n",
        "  ## convertion of np object to JSON serializable object\n",
        "  def convert(o):\n",
        "    if isinstance(o, np.float32): return float(o)  \n",
        "    raise TypeError\n",
        "\n",
        "template = 'acc:{:8.4f}%, cost:{:5f}'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZIcFLv3tve",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Progress bar class\n",
        "# @markdown Shows the job state: the current epoch, the phase (training or testing a set), and the analyzed samples\n",
        "class PROG:\n",
        "  def __init__(self, inc=1, length=100, info='setup'):\n",
        "    self.start = time()\n",
        "    self.val = 0\n",
        "    self.def_inc = inc\n",
        "    self.length = length\n",
        "    self.epoch = 0\n",
        "    self.info = info\n",
        "    self.out = display(self.progress(), display_id=True)\n",
        "  \n",
        "  def progress(self):\n",
        "    return HTML(\"\"\"\n",
        "      Run time: {time} <br>\n",
        "      <b> Epoch {epoch} - {info}</b> <br>\n",
        "      <progress\n",
        "        value='{value}'\n",
        "        max='{max}',\n",
        "        style='width: 50%'>\n",
        "      </progress> <br>\n",
        "    \"\"\".format(value=self.val, max=self.length, epoch=self.epoch,\n",
        "               info=self.info,\n",
        "               time=strftime(\"%H:%M:%S\", gmtime(time()-self.start)))\n",
        "    )\n",
        "\n",
        "  def set(self, info, length, val=0):\n",
        "    self.val = val\n",
        "    self.info = info\n",
        "    self.length = length\n",
        "    self.out.update(self.progress())\n",
        "\n",
        "  def inc(self, val=None):\n",
        "    self.val += self.def_inc if val is None else val\n",
        "    self.out.update(self.progress())\n",
        "\n",
        "#prog = PROG()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sibcbC0coG6",
        "colab_type": "text"
      },
      "source": [
        "#Configuration\n",
        "Load the default configuration applying the user choices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWAjrK0GGKQH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "'''\n",
        "method: recursive_edit\n",
        "parameter: dict0, dictionary to edit\n",
        "parameter: dict1, dictionary with new elements\n",
        "return: dict0, initial dict edited\n",
        "'''\n",
        "def recursive_edit (dict0, dict1):\n",
        "  for key, value in dict1.items():\n",
        "    if key in dict0: # edit only known parameters\n",
        "      if type(value) == dict: # enter in the sub dict to edit it\n",
        "        dict0[key] = recursive_edit(dict0[key], value)\n",
        "      else:\n",
        "        dict0[key] = value\n",
        "    else:\n",
        "      print(f'WARNING: parameter \"{key}\" not configurable, it has been discarded!')\n",
        "  return dict0\n",
        "\n",
        "def get_config(kwargs= {}):\n",
        "  config = {}\n",
        "\n",
        "  # General\n",
        "  config['dataset'] = 'mnist' # mnist / cifar10 / svhn\n",
        "  config['val_samples'] = 5000\n",
        "  config['seed'] = 1024 # None to not set\n",
        "  config['epochs'] = 300\n",
        "\n",
        "  # Network\n",
        "  config['net_size'] = 'shallow' # shallow / medium / deep\n",
        "  config['act'] = 'relu' # sigmoid / tanh / relu\n",
        "\n",
        "  # Training\n",
        "  config['batch_size'] = 100\n",
        "  #config['metric'] = 'accuracy' # acuracy / crossentropy\n",
        "  config['hyper_step'] = 10\n",
        "  config['opt'] = 'adam' # adam /sgd\n",
        "  config['lr'] = 1e-3\n",
        "  config['lr_decay'] = config['lr']/100\n",
        "\n",
        "  config['HP'] = {\n",
        "      'n0': {\n",
        "          'val': 0.3,\n",
        "          'train': True,\n",
        "          'opt': 'adam',\n",
        "          'lr': 1e-3\n",
        "      },\n",
        "      'n1': {\n",
        "          'val': 0.,\n",
        "          'train': False,\n",
        "          'opt': 'adam',\n",
        "          'lr': 1e-3\n",
        "      },\n",
        "      'L2': {\n",
        "          'val': -4.,\n",
        "          'density': 'model', # elem / model / layer\n",
        "          'train': True,\n",
        "          'opt': 'adam',\n",
        "          'lr': 1e-3\n",
        "      }\n",
        "  }\n",
        "  for hp in config['HP'].keys():\n",
        "    config['HP'][hp]['lr_decay'] = config['HP'][hp]['lr']/100\n",
        "\n",
        "  # configuration's customization\n",
        "  if len(kwargs):\n",
        "    config = recursive_edit(config, kwargs)\n",
        "\n",
        "  # NON-configurable data\n",
        "  config['sample_shape'] = (None,28,28,1) if config['dataset'] == 'mnist' else\\\n",
        "                   (None,32,32,3) #if config['dataset'] == 'cifar10'\n",
        "  config['h_units'] = [1000,1000,1000] if config['net_size'] == 'shallow' else\\\n",
        "                      [2000,1000,1000,500] if config['net_size'] == 'medium' else\\\n",
        "                      [4000,2000,1000,500,250] #if config['net_size'] == 'deep'\n",
        "  config['n_levels'] = len(config['h_units'])+2 # hinned + input + output\n",
        "  return config\n",
        "\n",
        "def export_config(path, config):\n",
        "  with open('{}/config.json'.format(path), 'w') as f:\n",
        "    f.write(json.dumps(config, default=convert, indent=4))\n",
        "\n",
        "def import_config(path):\n",
        "  with open('{}/config.json'.format(path)) as f:\n",
        "    config = json.load(f)\n",
        "  return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwqqxaxwbPBq",
        "colab_type": "text"
      },
      "source": [
        "#Dataset Load\n",
        "Load the selected dataset and divide it in batches and replicas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIg_YXyDVYC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  def train_scale(x,y):\n",
        "    x = tf.expand_dims(tf.cast(x, tf.float32),-1)/255\n",
        "    return x,y\n",
        "  def test_scale(x,y):\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    return x,y\n",
        "\n",
        "  def get_datasets(config, distribute=True):\n",
        "\n",
        "    (train_ds, test_ds), info = tfds.load(name=config['dataset'], split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "    train_ds = train_ds.map(train_scale)\n",
        "    test_ds = test_ds.map(test_scale)\n",
        "    \n",
        "    len_train = info.splits['train'].num_examples\n",
        "    #len_train = 6000 +config['val_samples']\n",
        "    len_test = info.splits['test'].num_examples\n",
        "\n",
        "    # DEBUG REDUCTION\n",
        "    train_ds = train_ds.take(len_train)\n",
        "    test_ds = test_ds.take(len_test)\n",
        "\n",
        "    DS = {} # DataSet collection\n",
        "    DS['batch'] = config['batch_size']*strategy.num_replicas_in_sync\n",
        "    DS['val'] = {'name':'Validation','size':config['val_samples'],'info':'_val  '}\n",
        "    DS['train'] = {'name':'Train','size':len_train-config['val_samples'],'info':'_train'}\n",
        "    DS['test'] = {'name':'Test','size':len_test,'info':'_test '}\n",
        "\n",
        "    # Split train in train & validation\n",
        "    train_ds = train_ds.shuffle(1024, seed=config['seed'], reshuffle_each_iteration=False)\n",
        "    val_ds = train_ds.take(config['val_samples'])\n",
        "    train_ds = train_ds.skip(config['val_samples'])\n",
        "\n",
        "    # Batch the Datasets\n",
        "    train_ds = train_ds.shuffle(1024).batch(DS['batch'])\n",
        "    test_ds = test_ds.shuffle(1024).batch(DS['batch'])\n",
        "    '''\n",
        "    batches_val = repeat * len_val / batch\n",
        "    batches_train = len_train / batch\n",
        "    beatch_val >= batches_train / hyper_step\n",
        "\n",
        "    repeat * len_val / batch >= len_train / (batch * hyper_step)\n",
        "    repeat >= len_train / (len_val * hyper_step)\n",
        "    '''\n",
        "    repeat = np.ceil(DS['train']['size']/(DS['val']['size']*config['hyper_step']))\n",
        "    val_ds = val_ds.repeat(repeat).shuffle(1024).batch(DS['batch'])\n",
        "    \n",
        "    if distribute:\n",
        "      DS['train']['ds'] = strategy.experimental_distribute_dataset(train_ds)\n",
        "      DS['val']['ds'] = strategy.experimental_distribute_dataset(val_ds)\n",
        "      DS['test']['ds'] = strategy.experimental_distribute_dataset(test_ds)\n",
        "    else: \n",
        "      DS['train']['ds'] = train_ds\n",
        "      DS['val']['ds'] = val_ds\n",
        "      DS['test']['ds'] = test_ds\n",
        "    return DS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHZKlnqibVQK",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgrsUiM050Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "\n",
        "  class MODEL(tf.keras.Model):\n",
        "\n",
        "    def input_noise(self, x, training=False):\n",
        "      if training: \n",
        "        self.n0_noise = tf.random.normal(tf.shape(x), stddev=self.n0)\n",
        "        x += self.n0_noise\n",
        "      return x\n",
        "    \n",
        "    def hidden_noise(self, x, training=False):\n",
        "      if training: \n",
        "        self.n1_noise.append(tf.random.normal(tf.shape(x), stddev=self.n1))\n",
        "        x += self.n1_noise[-1]\n",
        "      return x\n",
        "\n",
        "    def __init__(self, C):\n",
        "      super(MODEL, self).__init__()\n",
        "\n",
        "      self.opt_elem = f_OPT[C['opt']](learning_rate=C['lr'], decay=C['lr_decay'])\n",
        "      self.opt_HP = {}\n",
        "\n",
        "      self.n0_noise = ZERO\n",
        "      self.n1_noise = []\n",
        "\n",
        "      '''\n",
        "      Elementary parameter classification\n",
        "      par.name = level/layer/type\n",
        "      level: integer, level of th parameter, 0 is the input\n",
        "      layer: string, keras layer class [Dense, Dropout, BatchNorm, ...]\n",
        "      type: string, layer's parameter [kernel, bias, ... ]\n",
        "      '''\n",
        "\n",
        "      self.LAYERS = []\n",
        "      level = 0\n",
        "      # INPUT level\n",
        "      self.LAYERS.append(Flatten(name=f'{level}/flatten'))\n",
        "      if C['HP']['n0']['val'] or C['HP']['n0']['train']:\n",
        "        self.LAYERS.append(self.input_noise)\n",
        "\n",
        "      # HIDDEN levels\n",
        "      for units in C['h_units']:\n",
        "        level += 1\n",
        "        if C['HP']['n1']['val'] or C['HP']['n1']['train']:\n",
        "          self.LAYERS.append(self.hidden_noise)\n",
        "        self.LAYERS.append(Dense(units=units, activation=C['act'], name=f'{level}/dense'))\n",
        "        if not(C['HP']['L2']['val'] or C['HP']['L2']['train']): # without regularizer\n",
        "          self.LAYERS.append(BatchNormalization(name=f'{level}/Bnorm'))\n",
        "\n",
        "      # OUTPUT level\n",
        "      level += 1\n",
        "      self.LAYERS.append(Dense(units=10, activation=C['act'], name=f'{level}/dense'))\n",
        "      self.n_levels = level+1\n",
        "      \n",
        "      self.compile(optimizer = C['opt'],\n",
        "                  loss = self.compute_loss,\n",
        "                  metrics = ['accuracy'])\n",
        "      self.build(C['sample_shape'])\n",
        "\n",
        "      # ELEMENTARY PARAETERS\n",
        "      self.elem = self.trainable_variables\n",
        "\n",
        "      # HYPER PARAMETERS\n",
        "      self.HP_train = []\n",
        "      for hp, C_hp in C['HP'].items():\n",
        "        self.opt_HP[hp] = f_OPT[C_hp['opt']](learning_rate=C_hp['lr'], decay=C_hp['lr_decay'])\n",
        "        if C_hp['train']: self.HP_train.append(hp)\n",
        "\n",
        "        if hp == 'n0':\n",
        "          self.n0 = tf.Variable(C_hp['val'], True, constraint=lambda x: tf.clip_by_value(x,0,1),\n",
        "                                aggregation=tf.VariableAggregation.MEAN, name='n0')\n",
        "        elif hp == 'n1':\n",
        "          self.n1 = tf.Variable(C_hp['val'], True, constraint=lambda x: tf.clip_by_value(x,0,1),\n",
        "                                aggregation=tf.VariableAggregation.MEAN, name='n1')\n",
        "        elif hp == 'L2':\n",
        "          self.L2 = []\n",
        "          self.L2_density = C_hp['density']\n",
        "          if C_hp['val'] or C_hp['train']:\n",
        "            if C_hp['density'] == 'model':\n",
        "              self.L2.append(tf.Variable(C_hp['val'], dtype=tf.float32, \n",
        "                            aggregation=tf.VariableAggregation.MEAN, name='L2'))\n",
        "            elif C_hp['density'] == 'layer':\n",
        "              for i in range(self.n_levels):\n",
        "                self.L2.append(tf.Variable(C_hp['val'], dtype=tf.float32, \n",
        "                              aggregation=tf.VariableAggregation.MEAN, name=f'L2/{i}'))\n",
        "            elif C_hp['density'] == 'elem':\n",
        "              for el in self.elem:\n",
        "                self.L2.append(tf.Variable(C_hp['val'] * np.ones(tf.shape(el)),\n",
        "                              aggregation=tf.VariableAggregation.MEAN,\n",
        "                              dtype=tf.float32, name=f'L2/{el.name[:-2]}'))\n",
        "      \n",
        "      self.hyper = [self.n0, self.n1] + self.L2\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "      for layer in self.LAYERS:\n",
        "        x = layer(x, training=training)\n",
        "      return x\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred, batch_size, penalty=False):\n",
        "      single_loss = LOSS(tf.one_hot(y_true,10), y_pred)\n",
        "      # Compute loss that is scaled by sample_weight and by global batch size.\n",
        "      loss = tf.nn.compute_average_loss(single_loss, global_batch_size=batch_size)\n",
        "      # Add scaled regularization losses.\n",
        "      if penalty: loss += self.get_rgl_penalty()\n",
        "      return loss\n",
        "\n",
        "    def get_rgl_penalty(self):\n",
        "      if len(self.L2)==0: return 0\n",
        "      penalty = 0\n",
        "      if self.L2_density == 'model':\n",
        "        for par in self.elem:\n",
        "          penalty += tf.reduce_sum(par**2)\n",
        "        penalty *= 10**self.L2[0]\n",
        "      elif self.L2_density == 'layer':\n",
        "        for par in self.elem:\n",
        "          level = split_par_name(par,'level')\n",
        "          penalty += 10**self.L2[level] * tf.reduce_sum(par**2)\n",
        "      elif self.L2_density == 'elem':\n",
        "        for l2,par in zip(self.L2, self.elem):\n",
        "          penalty += tf.reduce_sum(tf.cast(10**l2,tf.float32) * par**2)\n",
        "      return tf.nn.scale_regularization_loss(penalty/2)\n",
        "    \n",
        "    def test(self, ds, prog_bar, batch_size):\n",
        "      METRIC2.reset_states()\n",
        "      loss_mean.reset_states()\n",
        "      prog_bar.set('Accuracy on '+ds['name']+' dataset', ds['size'])\n",
        "      for x in ds['ds']:\n",
        "        distributed_test_step(x, self, batch_size)\n",
        "        prog_bar.inc()\n",
        "      \n",
        "      return METRIC2.result().numpy(), loss_mean.result().numpy()\n",
        "\n",
        "    def getHP(self):\n",
        "      HP = []\n",
        "      if 'n0' in self.HP_train: HP += [str(self.n0.numpy())]\n",
        "      if 'n1' in self.HP_train: HP += [str(self.n1.numpy())]\n",
        "      if 'L2' in self.HP_train:\n",
        "        if self.L2_density == 'elem':\n",
        "          HP += [str(np.mean(par.numpy())) for par in self.L2]\n",
        "        else: HP += [str(par.numpy()) for par in self.L2]\n",
        "      return HP\n",
        "    \n",
        "    def get_lay_names(self):\n",
        "      names = []\n",
        "      for l in self.LAYERS: names.append(l.name)\n",
        "      return names\n",
        "  \n",
        "  # es      1           dense       kernel\n",
        "  n_part = {'level':0,  'layer':1,  'attr':2}\n",
        "  def split_par_name(par, *argv):\n",
        "    split = par.name[:-2].split('/')\n",
        "    split[0] = int(split[0])\n",
        "    if len(argv)==0: return split\n",
        "    elif len(argv)==1: return split[n_part[argv[0]]]\n",
        "    ret = []\n",
        "    for n in argv: ret += [split[n_part[n]]]\n",
        "    return ret\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTI09BTR5DXl",
        "colab_type": "text"
      },
      "source": [
        "# Step functions\n",
        "\n",
        "### 1. Train step\n",
        "Apply elementary parameters optimization over the train batch.\n",
        "\n",
        "### 2. Hyper step\n",
        "Apply hyper-parameters optimization with the given following gradient computation.\n",
        "\n",
        "\n",
        "### 3. Test step\n",
        "Compute loss over the test batch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ksyeotKTB7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  ##################\n",
        "  ### TRAIN STEP ###\n",
        "  def train_step(batch, model, batch_size):\n",
        "    # ELEMENTARY PARAMETERS\n",
        "    images, labels = batch\n",
        "    with tf.GradientTape() as g:\n",
        "      g.watch(model.elem)\n",
        "      preds = model(images, training=True)\n",
        "      C1 = model.compute_loss(labels, preds, batch_size, penalty=True)\n",
        "    gradC1_elem = g.gradient(C1, model.elem)\n",
        "    \n",
        "    model.opt_elem.apply_gradients(zip(gradC1_elem, model.elem))\n",
        "    METRIC1.update_state(labels, preds)\n",
        "    return C1\n",
        "\n",
        "  ##################\n",
        "  ### HYPER STEP ###\n",
        "  def hyper_step(batch2, model, batch_size, n_hidden):\n",
        "    x2,y2 = batch2\n",
        "    model.n1_noise = []\n",
        "\n",
        "    with tf.GradientTape(persistent=False) as g:\n",
        "      g.watch(model.elem)\n",
        "      pred2 = model(x2, training=True)\n",
        "      C2 = model.compute_loss(y2, pred2, batch_size)\n",
        "    gradC2_elem = g.gradient(C2, model.elem, unconnected_gradients='zero')\n",
        "\n",
        "    if 'n0' in model.HP_train:\n",
        "      gradC2_n0 = gradC2_elem[0] # ∇w1 C2\n",
        "      gradC2_n0 *= model.elem[0] # ∇a0 C2\n",
        "      gradC2_n0 = tf.reduce_mean(gradC2_n0, axis=1) # gradient mean over every pixel\n",
        "      gradC2_n0 *= tf.reduce_mean(model.n0_noise, axis=0) # noise mean over every pixel\n",
        "      gradC2_n0 = tf.reduce_sum(gradC2_n0) # gradient mean for the unique n0 value\n",
        "      if gradC2_n0==0: gradC2_n0 = tf.random.normal(tf.shape(gradC2_n0)) # prevent initial stack\n",
        "      model.opt_HP['n0'].apply_gradients(zip([gradC2_n0], [model.n0]))\n",
        "\n",
        "    if 'n1' in model.HP_train:\n",
        "      gradC2_n1 = ZERO\n",
        "      names = list(map(lambda x: x.name, model.elem))\n",
        "      for lev in range(n_hidden):\n",
        "        _grad = gradC2_elem[names.index(f'{lev+2}/dense/bias:0')] # ∇bn C2\n",
        "        _grad *= model.elem[names.index(f'{lev+2}/dense/kernel:0')] # ∇an C2\n",
        "        _grad = tf.reduce_mean(_grad, axis=1) # gradient mean over every pixel\n",
        "        _grad *= tf.reduce_mean(model.n1_noise[lev], axis=0) # noise mean over every pixel\n",
        "        _grad = tf.reduce_sum(_grad) # gradient mean for the unique n0 value\n",
        "        gradC2_n1 += _grad\n",
        "      if gradC2_n1==0: gradC2_n1 = tf.random.normal(tf.shape(gradC2_n1)) # prevent initial stack\n",
        "      model.opt_HP['n1'].apply_gradients(zip([gradC2_n1], [model.n1]))\n",
        "\n",
        "    if 'L2' in model.HP_train:\n",
        "      LR1 = model.opt_elem._decayed_lr(tf.float32)\n",
        "      # if model.L2_density == 'elem' + base for the other densities\n",
        "      gradC2_L2 = [-LR1*gr*el for gr,el in zip(gradC2_elem,model.elem)]\n",
        "      if model.L2_density == 'model':\n",
        "        gr_dense = ZERO\n",
        "        for gr in gradC2_L2: gr_dense += tf.reduce_sum(gr)\n",
        "        gradC2_L2 = [gr_dense]\n",
        "      elif model.L2_density == 'layer':\n",
        "        gr_dense = [ZERO for i in range(model.n_levels)]\n",
        "        #count = [0 for i in range(model.n_levels)]\n",
        "        for par, grad in zip(model.elem, gradC2_L2):\n",
        "          level = split_par_name(par, 'level')\n",
        "          gr_dense[level] += tf.reduce_sum(grad)\n",
        "          #count[level] += 1\n",
        "        #for i in range(model.n_levels): gr_dense[i] /= count[i]\n",
        "        gradC2_L2 = gr_dense\n",
        "      model.opt_HP['L2'].apply_gradients(zip(gradC2_L2, model.L2))\n",
        "\n",
        "  #################\n",
        "  ### TEST STEP ###\n",
        "  def test_step(batch, model, batch_size):\n",
        "    images, labels = batch\n",
        "    preds = model(images, training=False)\n",
        "    t_loss = model.compute_loss(labels, preds, batch_size)\n",
        "\n",
        "    METRIC2.update_state(labels, preds)\n",
        "    loss_mean.update_state(t_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jMybnP8Ndxt",
        "colab_type": "text"
      },
      "source": [
        "# Job execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLz8PNqk-4NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  CKPT_DIR = '%s/ckpt'\n",
        "\n",
        "  ######################\n",
        "  ### CREATE NEW RUN ###\n",
        "  def new_run(job_name=None, record=[], **kwargs):\n",
        "    if job_name is None: job_name = str(int(time()))\n",
        "    c = 0\n",
        "    while True:\n",
        "      if f'{job_name}_{c}' in os.listdir(): c += 1\n",
        "      else: \n",
        "        job_name = f'{job_name}_{c}'\n",
        "        break\n",
        "\n",
        "    config = get_config(kwargs)\n",
        "    model = MODEL(config)\n",
        "    print('Total layers:', len(model.LAYERS))\n",
        "    #model.summary()\n",
        "    os.mkdir(job_name)\n",
        "    os.mkdir(CKPT_DIR%job_name)\n",
        "    export_config(job_name, config)\n",
        "    return job_name, model, config, 1\n",
        "\n",
        "  ####################\n",
        "  ### LOAD OLD RUN ###\n",
        "  def load_run(job_name=None, record=[]):\n",
        "\n",
        "    with open(f'{job_name}/config.json','r') as f:\n",
        "      config = json.load(f)\n",
        "      config['sample_shape'] = tuple(config['sample_shape'])\n",
        "    \n",
        "    ckpt_path = tf.train.latest_checkpoint(CKPT_DIR%job_name)\n",
        "    print('Loading model from:',CKPT_DIR%job_name)\n",
        "    last_epoch = int(ckpt_path.split('_')[-1])\n",
        "    print('Last trained epoch:', last_epoch)\n",
        "\n",
        "    model = MODEL(config)\n",
        "    model.load_weights(ckpt_path).expect_partial()\n",
        "\n",
        "    #run(job_name, model, config, record, last_epoch)\n",
        "    return job_name, model, config, last_epoch\n",
        "\n",
        "  ####################\n",
        "  ### TRAIN & TEST ###\n",
        "  def run(job_name, model, config, record, epoch=0):\n",
        "    print('-----> JOB:', job_name,'<-----')\n",
        "    \n",
        "    DS = get_datasets(config)\n",
        "    prog_bar = PROG(DS['batch'])\n",
        "    info, info_display = {}, display({}, display_id=True)\n",
        "    template = 'acc:{:8.4f}%, cost:{:5f}'\n",
        "\n",
        "    # number of samples per epoch (used to compute speed)\n",
        "    samples_per_epoch = DS['train']['size'] + DS['val']['size']\n",
        "    for rec in record: samples_per_epoch += DS[rec]['size']\n",
        "\n",
        "    RECORDs = {}\n",
        "    RECORDs['metric'] = {}\n",
        "    RECORDs['cost'] = {}\n",
        "    RECORDs['HP'] = {}\n",
        "    RECORDs['HP_train'] = model.HP_train\n",
        "    RECORDs['train_time'] = time()\n",
        "\n",
        "    RECORDs['metric']['train'] = []\n",
        "    for rec in record: \n",
        "      RECORDs['metric'][rec] = []\n",
        "      RECORDs['cost'][rec] = []\n",
        "    for hyp in model.hyper: \n",
        "      RECORDs['HP'][hyp.name[:-2]] = [tf.reduce_mean(hyp).numpy()] # initial value\n",
        "    tf.random.set_seed(config['seed'])\n",
        "\n",
        "    # TRAIN LOOP\n",
        "    while True:\n",
        "      speed = time()\n",
        "      prog_bar.set(\"Training\", DS['train']['size'])\n",
        "      prog_bar.epoch = epoch\n",
        "      \n",
        "      num_batches = 0\n",
        "      val_bs = DS['val']['ds'].__iter__()\n",
        "      METRIC1.reset_states()\n",
        "      for train_b in DS['train']['ds']:\n",
        "        \n",
        "        num_batches += 1\n",
        "        C1 = distributed_train_step(train_b, model, DS['batch'])\n",
        "        info['ACC_train'] = METRIC1.result().numpy()\n",
        "\n",
        "        if num_batches%config['hyper_step'] == 0 and len(model.HP_train):\n",
        "          distributed_hyper_step(val_bs.next(), model, DS['batch'], len(config['h_units'])) # hyper step\n",
        "          info['HP'] = model.getHP()\n",
        "          info_display.update(info)\n",
        "        prog_bar.inc()\n",
        "\n",
        "      # RECORDs\n",
        "      for rec in record:\n",
        "        a,l = model.test(DS[rec], prog_bar, DS['batch'])\n",
        "        RECORDs['metric'][rec].append(a)\n",
        "        RECORDs['cost'][rec].append(l)\n",
        "        info[DS[rec]['info']] = template.format(a*100, l)\n",
        "      # during the training the accuracy of the train batches is computed,\n",
        "      # and summed up, to return the average at the end.\n",
        "      # if the accuracy over the whole train dataset at the end of epoch\n",
        "      # is not required, the average one is stored\n",
        "      if 'train' not in record:\n",
        "        RECORDs['metric']['train'].append(METRIC1.result().numpy())\n",
        "      for hyp in model.hyper:\n",
        "        RECORDs['HP'][hyp.name[:-2]].append(tf.reduce_mean(hyp).numpy())\n",
        "      \n",
        "      '''\n",
        "      # delete previous checkpoint and save the current one\n",
        "      for file in os.listdir(CKPT_DIR%job_name):\n",
        "        with open(f'{CKPT_DIR%job_name}/{file}','w') as f: f.write('')\n",
        "        os.remove(f'{CKPT_DIR%job_name}/{file}')\n",
        "      model.save_weights(f'{CKPT_DIR%job_name}/ckpt_{epoch+1}')\n",
        "      '''\n",
        "\n",
        "      info['time_per_sample'] = '{:.2e}s'.format((time() - speed) / samples_per_epoch)\n",
        "      info_display.update(info)\n",
        "      epoch += 1\n",
        "      if epoch == config['epochs']: break\n",
        "      # configurable arly stop? Taking in account the cost/acc?\n",
        "    \n",
        "    # TEST\n",
        "    RECORDs['train_time'] = time() - RECORDs['train_time']\n",
        "    a,l = model.test(DS['test'], prog_bar, DS['batch'])\n",
        "    RECORDs['metric']['final'] = a\n",
        "    RECORDs['cost']['final'] = l\n",
        "    RECORDs['epochs_trained'] = epoch\n",
        "    print(f'Final scores:',template.format(a*100, l))\n",
        "\n",
        "    with open('{}/record.json'.format(job_name), 'w') as f:\n",
        "      f.write(json.dumps(RECORDs, default=convert, indent=4))\n",
        "    model.save_weights(f'{CKPT_DIR%job_name}/ckpt_{epoch+1}')\n",
        "    return model, RECORDs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXOS9kj0HTfW",
        "colab_type": "text"
      },
      "source": [
        "#Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX6IdNowoR0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move to the project directory\n",
        "%cd /content/drive/My Drive/Colab Notebooks/NN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT3Dxh7Vdj7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = 'mnist'\n",
        "net = np.random.choice(['shallow','medium','deep'])\n",
        "record=['test']\n",
        "EPOCHS = 50\n",
        "\n",
        "HP = {\n",
        "  'n0': {\n",
        "    'val': float(np.round(np.random.uniform(0.,.9),2)),\n",
        "    'train': True,\n",
        "    'lr': 1e-3,\n",
        "  },\n",
        "  'L2': {\n",
        "    'val': float(np.round(np.random.uniform(-5.,-1.),2)),\n",
        "    'train': True,\n",
        "    'density': 'model', # elem / model / layer\n",
        "    'lr': 1e-3,\n",
        "  },\n",
        "  'n1': {\n",
        "    'val': 0.,#float(np.round(np.random.uniform(0.,.9),2)),\n",
        "    'train': False\n",
        "  }\n",
        "}\n",
        "\n",
        "print('net size:',net)\n",
        "print('epochs:',EPOCHS)\n",
        "print('HP:',json.dumps(HP, default=convert, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1-wTAg_Gn83",
        "colab_type": "text"
      },
      "source": [
        "## Single run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3AB0G0ovjLvI",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "\n",
        "  #for (n0,n1,l2,net) in itertools.product(_n0,_n1,_l2,_net):\n",
        "  for _ in range(1):\n",
        "    @tf.function\n",
        "    def distributed_train_step(batch, model, b_size):\n",
        "      per_replica_losses = strategy.run(train_step, args=(batch, model, b_size))\n",
        "      return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_hyper_step(batch, model, b_size, n_hidden):\n",
        "      strategy.run(hyper_step, args=(batch, model, b_size, n_hidden))\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_test_step(batch, model, b_size):\n",
        "      strategy.run(test_step, args=(batch, model, b_size))\n",
        "\n",
        "    JOB = ''\n",
        "    for _,hp in HP.items():\n",
        "      JOB += str(hp['val'])\n",
        "      JOB += 'T_' if hp['train'] else 'F_'\n",
        "    JOB += net[0]\n",
        "\n",
        "    # TRAIN + TEST\n",
        "    job_name, model, config, last_ep = new_run(job_name=JOB, record=record,\n",
        "              dataset=ds, epochs=EPOCHS, net_size=net, HP=HP)\n",
        "    model, REC = run(job_name, model, config, record, last_ep)\n",
        "\n",
        "    print('-'*40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSFg8n55GBIA",
        "colab_type": "text"
      },
      "source": [
        "## Multiple runs \n",
        "default → hyper training → optimized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GANhj7h1ArlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "\n",
        "  last_model = None\n",
        "  JOB = ''\n",
        "  for _,hp in HP.items():\n",
        "    JOB += str(hp['val'])\n",
        "    JOB += 'T_' if hp['train'] else 'F_'\n",
        "  JOB += f'{net[0]}_multi'\n",
        "  c = 0\n",
        "  while True:\n",
        "    if f'{JOB}_{c}' in os.listdir(): c += 1\n",
        "    else: break\n",
        "  JOB = f'{JOB}_{c}'\n",
        "  os.makedirs(JOB)\n",
        "  os.chdir(JOB)\n",
        "  print(JOB)\n",
        "\n",
        "  for name in ['default','trained','optimized']:\n",
        "    @tf.function\n",
        "    def distributed_train_step(batch, model, b_size):\n",
        "      per_replica_losses = strategy.run(train_step, args=(batch, model, b_size))\n",
        "      return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_hyper_step(batch, model, b_size, n_hidden):\n",
        "      strategy.run(hyper_step, args=(batch, model, b_size, n_hidden))\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_test_step(batch, model, b_size):\n",
        "      strategy.run(test_step, args=(batch, model, b_size))\n",
        "\n",
        "    # set trainable HPs\n",
        "    HP_run = HP\n",
        "    if name in ['default','optimized']:\n",
        "      for _,hp in HP_run.items():\n",
        "        hp['train'] = False\n",
        "\n",
        "    # load trained data\n",
        "    if name=='optimized':\n",
        "      model.n0 = last_model.n0\n",
        "      model.n1 = last_model.n1\n",
        "      model.L2 = last_model.L2\n",
        "\n",
        "    # RUN\n",
        "    job_name, model, config, last_ep = new_run(job_name=name, record=record,\n",
        "              dataset=ds, epochs=EPOCHS, net_size=net, HP=HP_run)\n",
        "    last_model, REC = run(job_name, model, config, record, last_ep)\n",
        "    if 'test' in REC['metric']: plt.plot(REC['metric']['test'], label=name)\n",
        "    print('-'*40)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  %cd '..'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMWx1bBhGU7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SMA(data, sma):\n",
        "  dataSMA = np.copy(data)\n",
        "  for i in range(1,sma): dataSMA += np.roll(data,i, axis=0)\n",
        "  dataSMA /= sma\n",
        "  dataSMA[:sma] = None\n",
        "  return dataSMA\n",
        "\n",
        "plotJOB = '0.8T_0.0F_-4.3T_s_multi_0'#JOB\n",
        "\n",
        "for n in ['default_0','trained_0','optimized_0']:\n",
        "  job_name, model, config, last_epoch = load_run(f'{plotJOB}/{n}')\n",
        "  with open(f'{plotJOB}/{n}/record.json') as f:\n",
        "    rec = json.load(f)\n",
        "  plt.plot(SMA(rec['metric']['test'],20), label=n)\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oXHvxU5w956",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}